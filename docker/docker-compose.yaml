x-python-executor: &python-executor
  build:
    context: .
    dockerfile: python.Dockerfile
  env_file: .env
  user: "${UID}:${GID}"
  volumes:
    - ../workspaces:/app/workspaces

services:
  main-llm:
    image: ghcr.io/ggml-org/llama.cpp:server-cuda
    container_name: main-llm-server
    env_file: .env
    ports:
      - "${MAIN_LLM_PORT}:${MAIN_LLM_PORT}"
    volumes:
      - ../models:/models
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    command: "-m ${MAIN_LLM_MODEL_PATH} --host 0.0.0.0 --port ${MAIN_LLM_PORT} -ngl ${N_GPU_LAYERS} -c 16384 --temp 0.1 --top_p 0.85 --top-k 30 --mlock --no-mmap --repeat-penalty 1.1"

  embedding-llm:
    image: ghcr.io/ggml-org/llama.cpp:server-cuda
    container_name: embedding-llm-server
    env_file: .env
    ports:
      - "${EMBEDDING_LLM_PORT}:${EMBEDDING_LLM_PORT}"
    volumes:
      - ../models:/models
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    command: >
      -m ${EMBEDDING_LLM_MODEL_PATH}
      --host 0.0.0.0
      --port ${EMBEDDING_LLM_PORT}
      --embedding
      -ngl ${N_GPU_LAYERS}
      -c 2048
      -b 512 
      --batch-size 2048
      --gpu-layers ${N_GPU_LAYERS}

  summarization-llm:
    image: ghcr.io/ggml-org/llama.cpp:server-cuda
    container_name: summarization-llm-server
    env_file: .env
    ports:
      - "${SUMMARIZATION_LLM_PORT}:${SUMMARIZATION_LLM_PORT}"
    volumes:
      - ../models:/models
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    # Factual and consistent settings for summarization
    command: >
      -m ${SUMMARIZATION_LLM_MODEL_PATH}
      --host 0.0.0.0
      --port ${SUMMARIZATION_LLM_PORT}
      -ngl ${N_GPU_LAYERS}
      --temp 0.2
      --top-p 0.9
      --top-k 40

  python-executor-1:
    <<: *python-executor
    container_name: python-executor-1
    ports:
      - "9999:9999"

  python-executor-2:
    <<: *python-executor
    container_name: python-executor-2
    ports:
      - "9998:9999"

  python-executor-3:
    <<: *python-executor
    container_name: python-executor-3
    ports:
      - "9997:9999"

  python-executor-4:
    <<: *python-executor
    container_name: python-executor-4
    ports:
      - "9996:9999"

  python-executor-5:
    <<: *python-executor
    container_name: python-executor-5
    ports:
      - "9995:9999"
