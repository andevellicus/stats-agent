x-python-executor: &python-executor
  build:
    context: ./executor
    dockerfile: python.Dockerfile
  env_file: .env
  # Remove user override - Dockerfile handles this with non-root user
  volumes:
    - ../workspaces:/app/workspaces
  restart: unless-stopped
  # Security constraints
  security_opt:
    - no-new-privileges:true
  cap_drop:
    - ALL
  cap_add:
    - CHOWN
    - SETUID
    - SETGID
  # Resource limits (prevent DOS)
  deploy:
    resources:
      limits:
        cpus: '2.0'
        memory: 4G
        pids: 512
      reservations:
        memory: 1G
  # Network isolation - no internet access
  # Comment out if your Python code needs to download data from internet
  network_mode: bridge
  # Uncomment to disable network entirely:
  # network_mode: none

services:
  postgres:
    image: pgvector/pgvector:pg15
    container_name: stats-agent-postgres
    env_file: .env
    environment:
      POSTGRES_DB: ${DB_NAME:-stats_agent}
      POSTGRES_USER: ${DB_USER:-postgres}
      POSTGRES_PASSWORD: ${DB_PASSWORD:-changeme}
    # Port removed - only accessible via Docker network (more secure)
    # If you need to connect from host for debugging, uncomment the line below:
    ports:
      - "5432:5432"
    volumes:
      - postgres-data:/var/lib/postgresql/data

  main-llm:
    image: ghcr.io/ggml-org/llama.cpp:server-cuda
    container_name: main-llm-server
    env_file: .env
    ports:
      - "${MAIN_LLM_PORT}:${MAIN_LLM_PORT}"
    volumes:
      - ../models:/models
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    command: >
      -m ${MAIN_LLM_MODEL_PATH}
      --host 0.0.0.0
      --port ${MAIN_LLM_PORT}
      -ngl ${N_GPU_LAYERS}
      -c 12288
      --top_p 0.85
      --top-k 40
      --repeat-penalty 1.1

  embedding-llm:
    image: ghcr.io/ggml-org/llama.cpp:server-cuda
    container_name: embedding-llm-server
    env_file: .env
    ports:
      - "${EMBEDDING_LLM_PORT}:${EMBEDDING_LLM_PORT}"
    volumes:
      - ../models:/models
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    command: >
      -m ${EMBEDDING_LLM_MODEL_PATH}
      --host 0.0.0.0
      --port ${EMBEDDING_LLM_PORT}
      --embedding
      -ngl ${N_GPU_LAYERS}
      -c 2048
      -b 512
      --batch-size 2048
      --mlock

  summarization-llm:
    image: ghcr.io/ggml-org/llama.cpp:server-cuda
    container_name: summarization-llm-server
    env_file: .env
    ports:
      - "${SUMMARIZATION_LLM_PORT}:${SUMMARIZATION_LLM_PORT}"
    volumes:
      - ../models:/models
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    # Factual and consistent settings for summarization
    command: >
      -m ${SUMMARIZATION_LLM_MODEL_PATH}
      --host 0.0.0.0
      --port ${SUMMARIZATION_LLM_PORT}
      -ngl ${N_GPU_LAYERS}
      --temp 0.05
      --top-p 0.5
      --top-k 20
      --repeat-penalty 1.2
      --repeat-last-n 128
      -c 4096

  python-executor-1:
    <<: *python-executor
    container_name: python-executor-1
    ports:
      - "9999:9999"

  python-executor-2:
    <<: *python-executor
    container_name: python-executor-2
    ports:
      - "9998:9999"

  python-executor-3:
    <<: *python-executor
    container_name: python-executor-3
    ports:
      - "9997:9999"

  python-executor-4:
    <<: *python-executor
    container_name: python-executor-4
    ports:
      - "9996:9999"

  python-executor-5:
    <<: *python-executor
    container_name: python-executor-5
    ports:
      - "9995:9999"

  pdf-extractor:
    build:
      context: ./pdf-extractor
      dockerfile: Dockerfile
    container_name: pdf-extractor
    ports:
      - "9001:9001"
    volumes:
      - ../workspaces:/app/workspaces
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  caddy:
    build:
      context: ./caddy
      dockerfile: Dockerfile
    container_name: stats-agent-caddy
    environment:
      # Set to your domain for automatic HTTPS (e.g. app.example.com)
      DOMAIN: ${DOMAIN:-localhost}
      # Upstream app address; default assumes app runs on host:5000
      UPSTREAM: ${UPSTREAM:-host.docker.internal:5000}
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./caddy/Caddyfile:/etc/caddy/Caddyfile:ro
      - caddy_data:/data
      - caddy_config:/config
    # Enable resolving host.docker.internal on Linux
    extra_hosts:
      - "host.docker.internal:host-gateway"
    restart: unless-stopped

volumes:
  postgres-data:
  caddy_data:
    name: caddy_data
  caddy_config:
    name: caddy_config
