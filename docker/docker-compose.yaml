services:
  main-llm:
    image: ghcr.io/ggml-org/llama.cpp:server-cuda
    container_name: main-llm-server
    env_file: .env # Load variables from the .env file
    ports:
      - "${MAIN_LLM_PORT}:${MAIN_LLM_PORT}" # Use the port variable
    volumes:
      - ../models:/models
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    command: "-m ${MAIN_LLM_MODEL_PATH} --host 0.0.0.0 --port ${MAIN_LLM_PORT} -ngl ${N_GPU_LAYERS}"

  embedding-llm:
    image: ghcr.io/ggml-org/llama.cpp:server-cuda
    container_name: embedding-llm-server
    env_file: .env
    ports:
      - "${EMBEDDING_LLM_PORT}:${EMBEDDING_LLM_PORT}"
    volumes:
      - ../models:/models
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    command: "-m ${EMBEDDING_LLM_MODEL_PATH} --host 0.0.0.0 --port ${EMBEDDING_LLM_PORT} --embedding -ngl ${N_GPU_LAYERS}"

  summarization-llm:
    image: ghcr.io/ggml-org/llama.cpp:server-cuda
    container_name: summarization-llm-server
    env_file: .env
    ports:
      - "${SUMMARIZATION_LLM_PORT}:${SUMMARIZATION_LLM_PORT}"
    volumes:
      - ../models:/models
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    command: "-m ${SUMMARIZATION_LLM_MODEL_PATH} --host 0.0.0.0 --port ${SUMMARIZATION_LLM_PORT} -ngl ${N_GPU_LAYERS}"

  python-executor:
    build:
      context: .
      dockerfile: python.Dockerfile
    container_name: python-executor-container
    env_file: .env # Also load the .env for UID/GID
    user: "${UID}:${GID}"
    ports:
      - "9999:9999"
    volumes:
      - ../workspace:/app/workspace